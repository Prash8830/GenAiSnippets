import pandas as pd
import numpy as np
import torch

import lightning.pytorch as pl
from pytorch_forecasting import (
    TimeSeriesDataSet,
    TemporalFusionTransformer,
    Baseline,
)
from pytorch_forecasting.data import GroupNormalizer
from pytorch_forecasting.metrics import QuantileLoss

# ============================================================
# 1. LOAD DATA
# ============================================================
data_path = "inventory_simulated_India_1yr.csv"   # adjust path if needed
df = pd.read_csv(data_path)

df["Date"] = pd.to_datetime(df["Date"])
df = df.sort_values(["Store_ID", "Product_ID", "Date"]).reset_index(drop=True)

# ============================================================
# 2. time_idx PER (Store_ID, Product_ID)
# ============================================================
df["time_idx"] = (
    df.groupby(["Store_ID", "Product_ID"])["Date"]
      .rank(method="dense")
      .astype(int) - 1
)

# ============================================================
# 3. DATE FEATURES (AS REAL VALUES)
# ============================================================
df["year"]         = df["Date"].dt.year
df["month"]        = df["Date"].dt.month
df["day_of_week"]  = df["Date"].dt.dayofweek
df["week_of_year"] = df["Date"].dt.isocalendar().week.astype(int)

# ============================================================
# 4. COLUMN DEFINITIONS
# ============================================================
target_col = "Units_Sold"
group_ids  = ["Store_ID", "Product_ID"]

categorical_cols = [
    "Store_ID",
    "Product_ID",
    "Category",
    "Brand",
    "Product",
    "Region",
    "Weather_Condition",
    "Holiday_Promotion",
    "Seasonality",
]

date_real_cols = ["year", "month", "day_of_week", "week_of_year"]

real_cols = [
    "Inventory_Level",
    "Units_Ordered",
    "Demand_Forecast",
    "Price",
    "Discount",
    "Competitor_Pricing",
    "Social_Media_Sentiment",
]

# ============================================================
# 5. MISSING VALUES
# ============================================================
for col in categorical_cols:
    df[col] = df[col].astype(str).fillna("Unknown")

for col in real_cols + [target_col]:
    df[col] = df[col].astype(float)
    df[col] = df[col].fillna(df[col].mean())

for col in date_real_cols:
    df[col] = df[col].astype(int)

for col in categorical_cols:
    df[col] = df[col].astype("category")

# ============================================================
# 6. TRAIN / VAL SPLIT
# ============================================================
df = df.sort_values("time_idx").reset_index(drop=True)
max_time = df["time_idx"].max()

max_prediction_length = 30    # forecast horizon
max_encoder_length    = 90    # history window

training_cutoff = max_time - max_prediction_length

# training set: up to cutoff
training_df = df[df["time_idx"] <= training_cutoff]

# for validation, follow PF style: use from_dataset with full data
# (we'll pass full df when building validation TimeSeriesDataSet)
print("Training rows:", training_df.shape[0])

# ============================================================
# 7. BUILD TRAINING TimeSeriesDataSet
# ============================================================
training_ds = TimeSeriesDataSet(
    training_df,
    time_idx="time_idx",
    target=target_col,
    group_ids=group_ids,
    max_encoder_length=max_encoder_length,
    max_prediction_length=max_prediction_length,
    target_normalizer=GroupNormalizer(groups=group_ids),

    time_varying_known_categoricals=categorical_cols,
    time_varying_known_reals=real_cols + date_real_cols + ["time_idx"],

    time_varying_unknown_reals=[target_col],

    add_relative_time_idx=True,
    add_target_scales=True,
    add_encoder_length=True,
)

# validation set: automatically pick last prediction windows from full df
validation_ds = TimeSeriesDataSet.from_dataset(
    training_ds,
    df,
    predict=True,
    stop_randomization=True,
)

# ============================================================
# 8. DATALOADERS (USE .to_dataloader LIKE DOCS)
# ============================================================
batch_size = 64

train_loader = training_ds.to_dataloader(
    train=True,
    batch_size=batch_size,
    num_workers=0,
)

val_loader = validation_ds.to_dataloader(
    train=False,
    batch_size=batch_size * 4,
    num_workers=0,
)

# ============================================================
# 9. BUILD TFT MODEL (FROM DATASET)
# ============================================================
pl.seed_everything(42)

quantiles = [0.1, 0.5, 0.9]       # or [0.5] if you want single
loss = QuantileLoss(quantiles=quantiles)

tft = TemporalFusionTransformer.from_dataset(
    training_ds,
    hidden_size=32,
    attention_head_size=1,
    dropout=0.1,
    hidden_continuous_size=16,
    loss=loss,
    output_size=len(quantiles),   # <<< match quantiles length
    learning_rate=3e-3,
    log_interval=10,
    reduce_on_plateau_patience=4,
)


print("Is LightningModule:", issubclass(type(tft), pl.LightningModule))
print(f"Number of parameters: {tft.size() / 1e3:.1f}k")

# ============================================================
# 10. TRAINER + FIT (lightning.pytorch API)
# ============================================================
trainer = pl.Trainer(
    max_epochs=30,
    accelerator="cpu",     # "gpu" if available
    gradient_clip_val=0.1,
)

trainer.fit(
    tft,
    train_dataloaders=train_loader,
    val_dataloaders=val_loader,
)

# ============================================================
# 11. OPTIONAL: BASELINE CHECK
# ============================================================
baseline = Baseline()
baseline_preds = baseline.predict(val_loader, return_y=True)
print("Baseline sample y:", baseline_preds.y[:5])
